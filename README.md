# andi_challenge

This repository contains code for the analysis of single particle tracking data for characterisation of anomalous diffusion, using convolutional neural networks (CNNs). 
Submitted to the [Anomalous Diffusion (AnDi) Challenge](http://www.andi-challenge.org/).

The models in this repository perform three tasks on 1D and 2D tracks, as set out in the challenge:
-	Task 1: inference of anomalous exponent
-	Task 2: classification of diffusion model
-	Task 3: segmentation of trajectories

The CNN architecture used here is based on that used in Granik et al., 2019 (ref below), and I thank the authors of that study for making their code freely available.
As well as some minor changes to hyperparameters and model training (as outlined in the code), I've made some major modifications to the original methods which I believe improve performance and applicability to real data:
- Consideration of tracks of all lengths (5-1000+ steps) with a single model 
- Training models on tracks with variable signal to noise ratio (permitting inference without prior information about SNR)
- Segmentation of trajectories switching between diffusion modes (currently limited to single length tracks with one switch, but I plan to extend this)

Note: the code in this repository has been modified and improved somewhat since the original challenge. For the exact code and models used in the original challenge, please see [this fork](https://github.com/AnDiChallenge/AnDi2020_TeamJ_FCI). 

## Installation

Clone the repository:

    git clone https://github.com/tsmbland/andi_challenge.git
    cd andi_challenge
    
Create conda environment:

    conda create -n andi python=3.7
    conda activate andi

Install packages:

    pip install -r requirements.txt


## Generating datasets

Validation and test data can be generated by running Test/Generate.py and Validation/Generate.py respectively.
By default, this will generate 10,000 tracks per task per dimension (1D and 2D) for test and validation, using the andi-datasets package.

Training data is generated on-the-fly during training, and does not need to be generated beforehand.


## Training models

To train models, run the scripts in the Train folder for the respective task (E.g. Task1_Exponent/Train/1D.py for task 1 in 1D).
Pre-trained models included as .h5 files in the Models folder for each task.


## Analysing challenge data

The repository includes three scripts for analysing challenge data in the ‘Challenge’ folder, named Task1.py, Task2.py and Task3.py for the three tasks. 
To perform analysis, run these scripts, specifying the path to the data folder at the top of the file. 

Non-challenge data (e.g. real data) can be analysed in a similar way!


## References

Granik, N., Weiss, L.E., Nehme, E., Levin, M., Chein, M., Perlson, E., Roichman, Y., and Shechtman, Y. (2019). Single-Particle Diffusion Characterization by Deep Learning. Biophys. J. 117, 185–192